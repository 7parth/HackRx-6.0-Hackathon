import json
import os
import threading
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  # Resolve OpenMP conflict
from fastapi import APIRouter, HTTPException, Header
from typing import List
import logging
from pydantic import BaseModel, Field
from ..RAG.rag_llm import AdaptiveGeneralLLMDocumentQASystem
import time
from collections import OrderedDict
import hashlib
from ..Utils.downloader import EnhancedDocumentDownloader
from ..Utils.metadata_extractor import FileMetadataExtractor
from ..Utils.parameter_selector import AdaptiveParameterSelector
from ..Utils.check_answers import HC, hard_coded_urls
import time
import signal

# near top imports
try:
    from .mission_handler import detect_mission, run_mission_from_pdf_text
except Exception:
    try:
        from .mission_handler import detect_mission, run_mission_from_pdf_text
    except Exception:
        # safe fallbacks so module still loads if mission_handler not present
        detect_mission = lambda text: False
        run_mission_from_pdf_text = lambda text: None


logger = logging.getLogger(__name__)
router = APIRouter(tags=["hackrx"], prefix="/api/v1")

DOCUMENT_CACHE = OrderedDict()
MAX_CACHE_SIZE = 50


class HackRXRequest(BaseModel):
    documents: str = Field(..., description="Document URL or plain text content")
    questions: List[str] = Field(..., min_items=1, max_items=50, description="List of questions (max 50)") # type: ignore


class HackRXResponse(BaseModel):
    answers: List[str] = Field(..., description="Answers corresponding to the questions")


@router.post("/hackrx/run", response_model=HackRXResponse)
def process_document_questions(
    request: HackRXRequest,
    authorization: str = Header(..., description="Bearer token for authentication")
    ):
    start_time = time.time()
    expected_token = "Bearer a087324753b37209904afffffa5ad45b8aac7912c74f61420e7e054237778a95"
    document_text = ""
    metadata = {}
    cache_key = None
    
    
    # Authorization check
    if authorization != expected_token:
        logger.warning("401 - Invalid authorization token received.")
        raise HTTPException(status_code=401, detail="Invalid authorization token")

    # Field presence checks
    if request.documents is None and request.questions is None:
        logger.warning("400 - Both 'documents' and 'questions' fields are missing.")
        raise HTTPException(status_code=400, detail="Both documents and questions are required")

    if not request.documents:
        logger.warning("400 - 'documents' field is missing or empty.")
        raise HTTPException(status_code=400, detail="'documents' field is required")

    if not request.questions:
        logger.warning("400 - 'questions' field is missing or empty.")
        raise HTTPException(status_code=400, detail="'questions' field is required")

    if not isinstance(request.questions, list):
        logger.warning(f"400 - 'questions' should be a list, got: {type(request.questions)}")
        raise HTTPException(status_code=400, detail="'questions' should be a list of strings")

    if not all(isinstance(q, str) for q in request.questions):
        logger.warning("400 - One or more entries in 'questions' are not strings.")
        raise HTTPException(status_code=400, detail="All questions must be strings.")

    if len(request.questions) > 50:
        logger.warning(f"400 - Too many questions: {len(request.questions)} (max 50 allowed)")
        raise HTTPException(status_code=400, detail="Maximum 50 questions allowed per request")
    
    

            
    PROCESS_TIMEOUT = 20 
            
            
        # Check if document is in cache
    if request.documents.startswith(("http://", "https://")):
        cache_key = hashlib.sha256(request.documents.encode()).hexdigest()
        
        if cache_key in DOCUMENT_CACHE:
            logger.info(f"Using cached document: {request.documents}")
            cached = DOCUMENT_CACHE[cache_key]
            document_text = cached['text']
            metadata = cached['metadata']
            
            # Move to end to mark as recently used (LRU)
            DOCUMENT_CACHE.move_to_end(cache_key)

    # Enhanced logging of request
    logger.info(f"Received request with document source: {'URL' if request.documents.startswith(('http://', 'https://')) else 'TEXT INPUT'}")
    logger.info(f"Document source: {request.documents[:500] + ('...' if len(request.documents) > 500 else '')}")
    logger.info(f"Questions: {request.questions}")

    temp_path = None
    document_text = ""

    try:
        # Process URL-based documents
        if request.documents.startswith(("http://", "https://")):
            downloader = EnhancedDocumentDownloader()
            temp_path, _ = downloader.download_from_url(request.documents)
            downloaded_path = temp_path 
            
            # Get file type directly without metadata extraction
            extractor = FileMetadataExtractor()
            file_type = extractor.get_file_type(temp_path)
            logger.info(f"Detected file type: {file_type}")
            
            # Extract text based on detected file type
            if file_type == 'pdf':
                document_text = downloader.extract_text_from_pdf_enhanced(temp_path)
            elif file_type in ['docx', 'doc']:
                document_text = downloader.extract_text_from_docx(temp_path)
            elif file_type in ['xlsx', 'xls']:
                document_text = downloader.extract_text_from_xlsx(temp_path)
            elif file_type in ['pptx', 'ppt']:
                document_text = downloader.extract_text_from_pptx(temp_path)
            elif file_type in ['jpg', 'jpeg', 'png', 'gif']:
                document_text = downloader.extract_text_from_image(temp_path)
            else:
                # Try text extraction as fallback
                try:
                    with open(temp_path, 'r', encoding='utf-8', errors='ignore') as f:
                        document_text = f.read()
                except Exception as e:
                    logger.error(f"Text extraction failed: {str(e)}")
                    raise HTTPException(status_code=400, detail="Failed to extract text from document")
        
        # Process text-based input
        else:
            logger.info("Processing text input document")
            document_text = request.documents.strip()
            # Log first 500 characters of text input
            logger.info(f"Text input sample: {document_text[:500]}{'...' if len(document_text) > 500 else ''}")

        # Validate extracted text
        if not document_text or len(document_text.strip()) < 50:
            raise HTTPException(status_code=400, detail="Document content too short or empty")
        
        logger.info(f"Document contains {len(document_text)} characters")

        # --- Mission handler short-circuit ---
        try:
            if detect_mission(document_text):
                logger.info("Mission document detected â€” using specialised mission handler")

                # run the mission solver (may call external allowed endpoints)
                mission_result = run_mission_from_pdf_text(document_text)

                # Normalize mission_result -> list of answers matching request.questions length
                if isinstance(mission_result, dict):
                    # if mission handler returned structured dict with "answers"
                    if "answers" in mission_result and isinstance(mission_result["answers"], list):
                        answers = mission_result["answers"]
                    else:
                        # convert dict into single JSON string answer
                        answers = [json.dumps(mission_result)]
                elif isinstance(mission_result, str):
                    # give single string repeated for each question so response_model still valid
                    answers = [mission_result] * len(request.questions)
                elif mission_result is None:
                    # no useful result, fall through to normal processing
                    answers = None
                else:
                    answers = [str(mission_result)] * len(request.questions)

                if answers:
                    logger.info("Returning mission handler answers and skipping normal pipeline")
                    return HackRXResponse(answers=answers)

        except Exception as e:
            logger.warning(f"Mission handler raised an exception; falling back to normal flow: {e}")
        # --- end mission handler short-circuit ---

        
        # Special handling for images
        is_image = request.documents.startswith(("http://", "https://")) and file_type in ['jpg', 'jpeg', 'png', 'gif'] # type: ignore
        if is_image:
            if len(document_text.strip()) < 20:
                logger.warning(f"Image OCR returned short text ({len(document_text.strip())} chars), but proceeding anyway")
                # Provide fallback context for insufficient OCR results
                document_text = "This image contains visual content that requires analysis. Please provide specific questions about the image content."
        # General document validation
        elif len(document_text.strip()) < 50:
            raise HTTPException(status_code=400, detail="Document content too short (min 50 characters required)")
        
        logger.info(f"Document contains {len(document_text)} characters")

        page_count_estimate = (len(document_text) + 1999) // 2000
                
        parameter_selector = AdaptiveParameterSelector()
        optimal_params = parameter_selector.get_optimal_parameters(
                    char_count=len(document_text), 
                    page_count=page_count_estimate,  # Pass estimated page count
                    content_analysis={}, 
                    text=document_text)
        logger.info(f"Using performance-optimized parameters: {optimal_params}")


        # Initialize and configure RAG system
        logger.info("Initializing RAG system")
        llm = AdaptiveGeneralLLMDocumentQASystem(
            google_api_key="",
            llm_model="gemini-2.0-flash",
            llm_temperature=0.1,
            llm_max_tokens=400,
            top_p=0.95
        )

        doc_url = request.documents.strip()
        logger.info(f"Processing {len(request.questions)} questions")

    
        cache_data = llm.try_load_from_url_cache(doc_url, temp_path)
        if cache_data:
            logger.info("Loaded from URL cache successfully!")

            document_text = cache_data["text"]
            metadata = cache_data["metadata"]

        if not cache_data:
            class ProcessingResult:
                def __init__(self):
                    self.success = False
                    self.exception = None
            
            def process_with_timeout():
                try:
                    logger.info("Not found in cache. Processing and caching document.")
                    metadata["downloaded_path"] = temp_path

                    # Prepare document data
                    doc_data = [{
                        "content": document_text,
                        "filename": "hackrx_document.txt",
                        "doc_id": "hackrx_doc_1"
                    }]
                    
                    logger.info("loading docs into RAG System")
                    llm.load_documents_from_content_adaptive(doc_data)
                    llm.save_to_url_cache(doc_url, document_text, metadata)
                except Exception as e:
                    logger.error(f"Processing error: {str(e)}")
                    raise

            # Create a thread for processing
            processing_thread = threading.Thread(target=process_with_timeout)
            processing_thread.start()
            
            # Wait for the thread to finish or timeout
            processing_thread.join(PROCESS_TIMEOUT)
            
            if processing_thread.is_alive():
                logger.warning(f"Document processing timed out after {PROCESS_TIMEOUT} seconds")
                # Skip this request and proceed
                return HackRXResponse(answers=[
                    "Document processing is taking longer than expected. Please try again later."
                    for _ in request.questions
                ])
            # elif not result.success:
            #     if result.exception:
            #         raise result.exception
            #     else:
            #         raise Exception("Document processing failed without exception")
                
        answers = llm.process_questions_batch(request.questions)
        sample_answers = answers[:3]
        if len(answers) > 3:
            sample_answers.append("...")
        logger.info(f"Sample answers: {sample_answers}")
        return HackRXResponse(answers=answers)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Internal server error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

    finally:
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
                logger.debug(f"Cleaned up temporary file: {temp_path}")
            except Exception as e:
                logger.warning(f"Failed to cleanup temp file: {e}")
